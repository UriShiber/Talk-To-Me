"""
this version got the first version of the func detect silence and an un
finish converter to wav
"""

import math
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import wavfile
import os
import wave
import pyaudio
from convert_audio_file_to_wave import convert_file_to_wav

RECORDING_DIR = r"C:\final_proj\Recordings"
FILES_TO_CONVERT_DIR = r"C:\final_proj\Files_To_Convert"


def compare(audio_file, audio_file2, volume_parameter):
    """
    :param audio_file: the new recording
    :param audio_file2: the old audio file which i compare to
    :param volume_parameter:
    :return: distance between them or an indicate on how they are similar to
             each other when 0 means they are identical.
    """
    distance = 0
    wave_file, chunk, stream, p = prepare_full_file(audio_file, True)
    data = wave_file.readframes(chunk)
    # the audio file which i compare to - the completed recording (that is in
    # the data base)
    wave_file2, chunk2, stream2, p2 = prepare_full_file(audio_file2, True)
    data2 = wave_file2.readframes(chunk2)
    try:
        # till the end of the file
        print(len(data), len(data2))
        while len(data) is not 0 and len(data2) is not 0:
            # taking the info in chunk of millisecond in np int.16 format
            data = np.fromstring(wave_file.readframes(chunk), dtype=np.int16)
            data2 = np.fromstring(wave_file2.readframes(chunk2), dtype=np.int16)
            # calculates a num for a margin of the highest part and lowest parts
            # by taking the absolute value for each member, then
            peak = np.average(np.abs(data))
            peak2 = np.average(np.abs(data2))
            peak2 = int(peak2) * volume_parameter
            # this is the function, could get more efficient
            distance += math.pow((int(peak2) - int(peak)), 2)
    except ValueError:
        print('finished comparing')
    # stops the stream
    stream.stop_stream()
    stream.close()
    p.terminate()

    stream2.stop_stream()
    stream2.close()
    p2.terminate()
    return distance


def detect_volume_parameter(audio_chunk):
    """
    volume is represented by the sum of the peak's values (each chunk - each
    millisecond value) divided by the number of chunks (duration of the word).
    the volume parameter is represented by 1000 / volume.
    in order to get the normalized volume we need later to multiply each peak
    with the volume_parameter, as a result, the averaged volume will always be
    1000, for example the initial volume is 500, so the volume parameter will be
    2, we later multiply each peak by two, as a result the averaged volume will
    be 1000
    :param audio_chunk:
    :return:
    """
    wave_file, chunk, stream, p = prepare_full_file(audio_chunk, False)
    data = wave_file.readframes(chunk)
    counter = 0
    volume = 0
    volume_parameter = 0
    try:
        # till the end of the file
        while len(data) is not 0:
            counter += 1
            # taking the info in chunk of millisecond in np int.16 format
            data = np.fromstring(wave_file.readframes(chunk), dtype=np.int16)
            # calculates a num for a margin of the highest part and lowest parts
            # by taking the absolute value for each member, then
            # calculates the average
            peak = np.average(np.abs(data))
            volume += int(peak)
    except ValueError:
        volume = volume / counter
        # choose 1000 as the normal volume per peak
        volume_parameter = 1000 / volume

    # stops the stream
    stream.stop_stream()
    stream.close()
    p.terminate()
    return volume_parameter


def detect_silence(audio_file):
    """
    the len normalizer is in prepare_full_file
    detect spaces.
    first we divide a file to milliseconds - for comfort, we see for each chunk
    if its peak is more than 380 (roughly the volume of speech in these kinds of
    measuring), we write to a new file we open for each word thus (according to
    different kinds of limitations, such as len of silence=100, the norma).
    we get the audio file split to spaces in separate wav files.
    and we get the start and end indexes of a word - for debugging
    letters that are hard to manage (when opening a word):
    f
    heit
    chaf
    sameh
    shin
    """
    wave_file, chunk, stream, p = prepare_full_file(audio_file,
                                                    bool_normalize_duration=
                                                    False)
    # detects the volume parameter of each record file in order to normalize
    # the audio chunks we get out of it to a 1000 volume
    data = wave_file.readframes(chunk)
    # represents the time of the words ([start, end....]) for debugging
    words_time = []
    num_of_files = 0
    in_a_word = False
    # represents the min len of silence (counts from 0 till 1000)
    times_under_400 = 0
    # represents the milliseconds
    counter = 0
    # if it is the first file i open
    first = True
    # before the loop, the first file must be already declared and opened
    new_wav_file = wave.open(os.path.join(RECORDING_DIR, ("audio_chunk{0}.wav".
                                          format(num_of_files))), 'wb')
    new_wav_file.setparams((wave_file.getnchannels(),
                            wave_file.getsampwidth(),
                            wave_file.getframerate(),
                            wave_file.getnframes(),
                            wave_file.getcomptype(),
                            wave_file.getcompname()))
    try:
        # till the end of the file
        while data is not '':
            counter += 1
            # taking the info in chunk of millisecond in np int.16 format
            data = np.fromstring(wave_file.readframes(chunk), dtype=np.int16)
            # calculates a num for a margin of the highest part and lowest parts
            # by taking the absolute value for each member, then
            # calculates the average
            peak = np.average(np.abs(data))
            # normalization
            if not in_a_word:
                if peak > 400:
                    in_a_word = True
                    words_time.append(counter)
                    # if this is the second file, i need to open a new one
                    if not first:
                        num_of_files += 1
                        new_wav_file = wave.open(os.path.join
                                                 (RECORDING_DIR,
                                                  ("audio_chunk{0}.wav".format
                                                   (num_of_files))), 'wb')
                        new_wav_file.setparams((wave_file.getnchannels(),
                                                wave_file.getsampwidth(),
                                                wave_file.getframerate(),
                                                wave_file.getnframes(),
                                                wave_file.getcomptype(),
                                                wave_file.getcompname()))
                    # writes the sound to the new file (for each millisecond)
                    # (the start of a new word)
                    new_wav_file.writeframesraw(data)
            if in_a_word:
                # in a word, so write the sound to the new file till you get out
                # of the word
                new_wav_file.writeframesraw(data)
                if peak < 400:
                    times_under_400 += 1
                    if times_under_400 >= 175:  # len of silence
                        in_a_word = False
                        first = False
                        words_time.append(counter)
                        times_under_400 = 0
                else:
                    times_under_400 = 0
            # for debugging - good indicate for the volume
            bars = "#" * int(1000 * int(peak) / 2 ** 16)
            print("%04d %05d %s" % (counter, peak, bars))
    except ValueError:
        print('finished')
        print(words_time)
    # stops the stream
    stream.stop_stream()
    stream.close()
    p.terminate()
    return words_time.__len__() / 2


def prepare_full_file(audio_file, bool_normalize_duration):
    """

    a problam with the duration normalizer !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


    in order to normalize the duration, needs to be called with a file name and
    true (otherwise calculates regularly):
    600 milliseconds is approximately the duration of a word, as a result i want
    that the duration of all the files will be as if it was 600 so
    each chunk will represent the data of each 1/600 instead of each millisecond
    for example if word's duration is 477, each chunk length will be
    48 (normal proportional chunk size in millisecond) * the duration parameter
    which is duration / 600 which means that every chunk will represent the num
    of frames / 600
    """
    wave_file = wave.open(audio_file, 'rb')
    duration = wave_file.getnframes() / float(wave_file.getframerate())
    duration = duration * 1000
    # the rate of an audio file means the number of updates or chunks in seconds
    # divide that by 1000 and we will get that each chunk will be a millisecond
    # i want that each chunk will be in the size of a millisecond (like in the
    # graph i drew earlier)
    if bool_normalize_duration:
        duration_parameter = duration / 600
        chunk = round(float(wave_file.getframerate() * duration_parameter)
                      / 1000)
    else:
        chunk = round(float(wave_file.getframerate()) / 1000)
    p = pyaudio.PyAudio()
    stream = p.open(format=p.get_format_from_width(wave_file.getsampwidth()),
                    channels=wave_file.getnchannels(),
                    rate=wave_file.getframerate(),
                    output=True)
    return wave_file, chunk, stream, p


def prepare_for_graphs(audio_file):
    # for drawing the graphs====================================================
    frequency_sampling, audio_signal = wavfile.read(audio_file)
    # show characteristics to me
    print('Signal shape:', audio_signal.shape)
    print('Signal Datatype:', audio_signal.dtype)
    print('Signal duration:', round(audio_signal.shape[0] /
                                    float(frequency_sampling), 2), 'seconds')
    # normalizes the audio signal
    audio_signal = (audio_signal / np.power(2, 15))
    # first part - graph of amplitude to time ----------------------------------
    time_axis = amplitude_to_time(audio_signal, frequency_sampling)
    draw(time_axis, audio_signal, 'Time (milliseconds)', 'Amplitude')
    # second part - graph of power to frequency --------------------------------
    x_axis, signal_power = power_to_frequency(audio_signal, frequency_sampling)
    draw(x_axis, signal_power, 'Frequency (kHz)', 'Signal power (dB)')
    # third part (to do) -------------------------------------------------------
    # ==========================================================================


def draw(x_axis, audio, x_label, y_label):
    plt.plot(x_axis, audio, color='blue')
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.title('Input audio signal')
    plt.show()


def amplitude_to_time(audio_signal, frequency_sampling):
    """
    does the first part in the paper - preparing the audio signal to be drawn.
    """
    # audio_signal = audio_signal[:100]
    time_axis = 1000 * np.arange(0, len(audio_signal), 1) /\
        float(frequency_sampling)
    return time_axis


def power_to_frequency(audio_signal, frequency_sampling):
    """
    need to figure out what it does
    :param audio_signal:
    :param frequency_sampling:
    :return:
    """
    length_signal = len(audio_signal)
    half_length = np.ceil((length_signal + 1) / 2.0).astype(np.int)
    signal_frequency = np.fft.fft(audio_signal)
    signal_frequency = abs(signal_frequency[0:half_length]) / length_signal
    signal_frequency **= 2
    len_fts = len(signal_frequency)
    if length_signal % 2:
        signal_frequency[1:len_fts] *= 2
    else:
        signal_frequency[1:len_fts-1] *= 2
    try:
        signal_power = 10 * np.log10(signal_frequency)
    except RuntimeWarning:
        signal_power = 0
    x_axis = np.arange(0, len_fts, 1) * (frequency_sampling / length_signal) /\
        1000.0
    return x_axis, signal_power


def chunks_names(num_of_chunks):
    counter = 1
    while counter <= num_of_chunks:
        file_name = os.path.join(RECORDING_DIR, "audio_chunk{0}.wav"
                                 .format(counter))
        volume_parameter = detect_volume_parameter(file_name)
        distance = compare(r"C:\final_proj\Recordings\audio_chunk4.wav",
                           file_name, volume_parameter)
        print("distance:------------------------------- ", distance, " -------")
        counter += 1


def separates_to_words():
    # creating a wav file from a recording -------------------------------------
    # audio_file = convert_file_to_wav(os.path.join(FILES_TO_CONVERT_DIR,
                                                  # "hebrew c.m4a"))

    # separating the audio into words ------------------------------------------
    # num_of_chunks = detect_silence(audio_file)
    # --------------------------------------------------------------------------
    # normalizes ---------------------------------------------------------------
    chunks_names(3)
    # --------------------------------------------------------------------------
    # drawing the graphs (amplitude to time and power to frequency)-------------
    # prepare_for_graphs(audio_file)
    # --------------------------------------------------------------------------


if __name__ == '__main__':
    separates_to_words()
